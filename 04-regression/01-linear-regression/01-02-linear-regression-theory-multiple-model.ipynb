{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ad579b-8a47-4cda-9b65-aa3298fe1490",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#daee8420; line-height:1.5; text-align:center;border:2px solid black;\">\n",
    "    <div style=\"color:#7B242F; font-size:24pt; font-weight:700;\">The Ultimate Machine Learning Mastery Course with Python</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b41d2-c8aa-4b60-a428-a3d0038bba10",
   "metadata": {},
   "source": [
    "---\n",
    "### **Course**: The Ultimate Machine Learning Course with Python  \n",
    "#### **Chapter**: Regression  \n",
    "##### **Lesson**: Linear Regression Theory  \n",
    "###### **Author:** Dr. Saad Laouadi   \n",
    "###### **Copyright:** Dr. Saad Laouadi    \n",
    "\n",
    "---\n",
    "\n",
    "## License\n",
    "\n",
    "**This material is intended for educational purposes only and may not be used directly in courses, video recordings, or similar without prior consent from the author. When using or referencing this material, proper credit must be attributed to the author.**\n",
    "\n",
    "```text\n",
    "#**************************************************************************\n",
    "#* (C) Copyright 2024 by Dr. Saad Laouadi. All Rights Reserved.           *\n",
    "#**************************************************************************                                                                    \n",
    "#* DISCLAIMER: The author has used their best efforts in preparing        *\n",
    "#* this content. These efforts include development, research,             *\n",
    "#* and testing of the theories and programs to determine their            *\n",
    "#* effectiveness. The author makes no warranty of any kind,               *\n",
    "#* expressed or implied, with regard to these programs or                 *\n",
    "#* to the documentation contained within. The author shall not            *\n",
    "#* be liable in any event for incidental or consequential damages         *\n",
    "#* in connection with, or arising out of, the furnishing,                 *\n",
    "#* performance, or use of these programs.                                 *\n",
    "#*                                                                        *\n",
    "#* This content is intended for tutorials, online articles,               *\n",
    "#* and other educational purposes.                                        *\n",
    "#**************************************************************************\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab48b1e-45b2-46e8-9fd8-1eb103182a30",
   "metadata": {},
   "source": [
    "# Comprehensive Lesson on Multiple Linear Regression\n",
    "\n",
    "### Overview\n",
    "\n",
    "Multiple Linear Regression (MLR) is an extension of Simple Linear Regression where more than one independent variable is used to predict the dependent variable. The goal remains the same: to model the relationship between the independent variables and the dependent variable by fitting a linear equation.\n",
    "\n",
    "\n",
    "### 1. **General Mathematical Formula**\n",
    "\n",
    "In Multiple Linear Regression, the predicted value ($\\hat{y}$) is a linear combination of multiple independent variables ($x_1, x_2, ..., x_p$):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + ... + \\hat{\\beta_p} x_p\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ is the predicted value (dependent variable).\n",
    "- $x_1, x_2, ..., x_p$ are the independent variables.\n",
    "- $\\hat{\\beta_0}$ is the intercept (the value of $\\hat{y}$ when all $x$ values are zero).\n",
    "- $\\hat{\\beta_1}, \\hat{\\beta_2}, ..., \\hat{\\beta_p}$ are the estimated coefficients for the independent variables.\n",
    "\n",
    "\n",
    "### 2. **Model Representation**\n",
    "\n",
    "The general formula for multiple linear regression is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the actual observed value (dependent variable).\n",
    "- $\\epsilon$ is the error term (or residual) representing the difference between the actual and predicted values.\n",
    "\n",
    "The goal is to find the best-fitting coefficients $\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\beta_2}, ..., \\hat{\\beta_p}$ that minimize the sum of squared residuals (errors).\n",
    "\n",
    "### Representing the Data in Matrix Notation\n",
    "\n",
    "Multiple Linear Regression can be represented concisely in matrix form, which is particularly useful for computation and deriving the coefficients.\n",
    "\n",
    "Letâ€™s represent the model as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{y}$ is the $n \\times 1$ vector of observed values (dependent variable).\n",
    "  \n",
    "  $$\n",
    "  \\mathbf{y} =\n",
    "  \\begin{bmatrix}\n",
    "  y_1 \\\\\n",
    "  y_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  y_n\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- $\\mathbf{X}$ is the $n \\times (p+1)$ matrix of independent variables, including a column of ones for the intercept.\n",
    "\n",
    "  $$\n",
    "  \\mathbf{X} =\n",
    "  \\begin{bmatrix}\n",
    "  1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "  1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
    "  1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- $\\boldsymbol{\\beta}$ is the $(p+1) \\times 1$ vector of coefficients (including the intercept).\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{\\beta} =\n",
    "  \\begin{bmatrix}\n",
    "  \\beta_0 \\\\\n",
    "  \\beta_1 \\\\\n",
    "  \\beta_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  \\beta_p\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- $\\boldsymbol{\\epsilon}$ is the $n \\times 1$ vector of errors or residuals.\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{\\epsilon} =\n",
    "  \\begin{bmatrix}\n",
    "  \\epsilon_1 \\\\\n",
    "  \\epsilon_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  \\epsilon_n\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "### Coefficient Estimation in Matrix Form\n",
    "\n",
    "The Ordinary Least Squares (OLS) method gives the estimated coefficients $\\hat{\\boldsymbol{\\beta}}$ as:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{X}^T$ is the transpose of the matrix $\\mathbf{X}$.\n",
    "- $(\\mathbf{X}^T \\mathbf{X})^{-1}$ is the inverse of the product of $\\mathbf{X}^T$ and $\\mathbf{X}$.\n",
    "  \n",
    "This matrix formulation provides a compact and efficient way to compute the regression coefficients using linear algebra techniques.\n",
    "\n",
    "\n",
    "### 3. **Derivation of Parameters**\n",
    "\n",
    "To estimate the coefficients $\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\beta_2}, ..., \\hat{\\beta_p}$, we again use the **Ordinary Least Squares (OLS)** method. The OLS approach aims to minimize the sum of squared errors (SSE):\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip}))^2\n",
    "$$\n",
    "\n",
    "The solution to this minimization problem involves matrix algebra and can be represented as:\n",
    "\n",
    "#### Matrix Form of Multiple Linear Regression\n",
    "\n",
    "The multiple linear regression model can be written in matrix form as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{y}$ is an $n \\times 1$ vector of the dependent variable values.\n",
    "- $\\mathbf{X}$ is an $n \\times (p+1)$ matrix of the independent variables (including a column of 1s for the intercept).\n",
    "- $\\boldsymbol{\\beta}$ is a $(p+1) \\times 1$ vector of the coefficients ($\\beta_0, \\beta_1, ..., \\beta_p$).\n",
    "- $\\boldsymbol{\\epsilon}$ is an $n \\times 1$ vector of the residuals (errors).\n",
    "\n",
    "The estimated coefficients ($\\hat{\\boldsymbol{\\beta}}$) are computed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "### 4. **Evaluating the Model**\n",
    "\n",
    "As with simple linear regression, multiple linear regression models are evaluated using various statistical metrics to assess the fit and accuracy of the model.\n",
    "\n",
    "#### a. **Coefficient of Determination ($R^2$)**:\n",
    "\n",
    "The $R^2$ value measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "#### b. **Adjusted $R^2$**:\n",
    "\n",
    "Adjusted $R^2$ is a modified version of $R^2$ that accounts for the number of independent variables in the model. It is used to prevent overfitting.\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of observations.\n",
    "- $p$ is the number of independent variables.\n",
    "\n",
    "#### c. **Mean Squared Error (MSE)**:\n",
    "MSE measures the average of the squared differences between actual and predicted values.\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "#### d. **Root Mean Squared Error (RMSE)**:\n",
    "RMSE is the square root of MSE and is used to measure the average magnitude of the errors in the predictions.\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "### 5. **Example of Multiple Linear Regression**\n",
    "\n",
    "Letâ€™s consider an example where we want to predict a house price based on two independent variables: **size (square footage)** and **number of bedrooms**.\n",
    "\n",
    "| Size (sq ft) | Bedrooms | Price (\\$1000s) |\n",
    "|--------------|----------|-----------------|\n",
    "| 1400         | 3        | 300             |\n",
    "| 1600         | 3        | 330             |\n",
    "| 1700         | 4        | 355             |\n",
    "| 1875         | 3        | 375             |\n",
    "| 1100         | 2        | 225             |\n",
    "\n",
    "The multiple linear regression model is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot \\text{Size} + \\hat{\\beta_2} \\cdot \\text{Bedrooms}\n",
    "$$\n",
    "\n",
    "#### Step 1: Calculate the coefficients ($\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\beta_2}$)\n",
    "\n",
    "Using OLS, we compute the best-fitting coefficients for the model.\n",
    "\n",
    "#### Step 2: Prediction Formula\n",
    "\n",
    "Once the coefficients are determined, we can predict house prices for new data points using the formula:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot \\text{Size} + \\hat{\\beta_2} \\cdot \\text{Bedrooms}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Assumptions of Multiple Linear Regression**\n",
    "\n",
    "Just like in Simple Linear Regression, Multiple Linear Regression also relies on several assumptions:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable is linear.\n",
    "2. **Independence**: Observations should be independent of each other.\n",
    "3. **Homoscedasticity**: The residuals have constant variance at every level of the independent variables.\n",
    "4. **Normality**: The residuals are normally distributed.\n",
    "5. **No Multicollinearity**: The independent variables are not too highly correlated with each other.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Multiple Linear Regression extends the power of simple linear regression by using more than one independent variable. It allows us to model more complex relationships and capture the influence of multiple factors on the dependent variable. By understanding the derivation and evaluation of the model, we can apply multiple linear regression to various real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a50183-98b7-44f4-8cdd-df1879b5e7f3",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [Newcastle University](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/simple-linear-regression.html)\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34791e-1850-41ba-8cfa-ad79a3db6f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
